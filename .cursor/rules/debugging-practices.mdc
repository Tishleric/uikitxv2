---
description: 
globs: 
alwaysApply: true
---
# Debugging Practices

## High-Impact Principles

- **Fail fast, fail loud:** It's better for software to crash or raise an error when something goes wrong than to silently continue in a corrupted state. Use assertions and sanity checks to catch impossible conditions immediately. In production, crashing with logs or error reports is often preferable to "random" wrong behavior – you get a clear signal that something needs fixing.  
- **Instrumentation and observability:** Build in logging, metrics, and tracing from the start. A well-placed log (at INFO or DEBUG level) can save hours when diagnosing an incident. *"Saving tracebacks to a log file can make it easier to debug problems… hard to reproduce… in production."* Use structured logging and include context (identifiers, timestamps, etc.) so you can trace what happened when things go wrong.  
- **Reproduce issues systematically:** When a bug appears, make it your goal to reproduce it reliably. *"Reproduce it!"* – find a minimal set of steps or a data sample that triggers the bug every time. Once you can do that, you have a golden key to debug: you can experiment and know if you've fixed it. If the bug cannot be reproduced easily, enhance logging or add debug modes to gather more info next time.  
- **Stay calm and methodical:** In a fire-fight (production outage, big bug), the worst thing is to panic or thrash. Take a deep breath and make a plan. *"Don't Panic."* Instead of making random changes, systematically narrow the problem. Use binary search through configurations or code (disable half the features to see if the bug remains, then narrow down). This analytical approach prevents injecting new errors under duress.  
- **Root cause, not symptoms:** Keep asking "why" until you get to the fundamental cause of a bug. For example, don't stop at "X was null and caused a crash; I'll just add a null-check." Ask why X was null – was there an earlier logic error or unexpected input? Aim to fix that root cause. *"Fix the problem, not the blame."* – focus on the technical issue, not whose fault it was. After solving, consider writing a test to cover this scenario in the future.  
- **Prepare for "impossible" cases:** If you ever think "this bug is impossible," remind yourself that the software is telling you otherwise. Accept that something unexpected happened and use that as a clue. Sometimes the bug is in an assumption you believed was infallible. In design, include safety nets: e.g., default cases in switches, else clauses, or error handlers for states you "think" are impossible – just in case. They can always alert if truly never reached.  
- **Leverage debugging tools:** Use interactive debuggers (like `pdb` in Python, or IDE breakpoints) to inspect state at runtime. A debugger lets you pause the program at the right spot and examine variables, step through code line by line, and truly *see* what is happening. This can be far more efficient than adding dozens of print statements. In production, tools like error monitoring dashboards or remote debuggers can be invaluable to peek at a live system's state (with care for performance and security).  
- **Minimal invasive "surgery" in prod:** If you must apply a quick fix directly in a production environment (e.g., a hotfix or a manual data correction), keep it as surgical as possible. Change the minimum necessary and document exactly what was done. Ideally, feature-flag problematic sections off. This reduces the risk of the fix causing more issues. Then, as soon as possible, follow up with a thorough fix via the normal development pipeline.  
- **Design for debugging:** Write code and design systems with debuggability in mind. For instance, assign correlation IDs to requests so you can trace a single transaction through logs, or design state machines such that they can dump their current state easily. If a module is complex, provide a debug interface or verbose mode. Simple things like meaningful error messages (including context like which user or data caused an error) go a long way in production troubleshooting.  
- **Post-mortem culture:** After a major bug or outage, do a *post-mortem*. Gather what happened, how it was fixed, and how to prevent it. This is not for blame, but for learning. Often this leads to action items: improve monitoring here, add a test there, adjust the process somewhere else. Over time, this practice drastically improves software reliability and team expertise.

## Antipatterns to Avoid

- **Ignoring the obvious signals:** Don't disregard error messages, stack traces, or alerts. The program often tells you what's wrong (*exception message, error code, log entry*). For example, skipping over an exception's text is a missed opportunity – always **"Read the Damn Error Message."** Many bugs can be solved by carefully reading and understanding the first error that occurred.  
- **Scattershot guessing:** Avoid the habit of randomly changing code or configurations hoping the bug goes away. This "shooting in the dark" may hide the symptom without addressing cause, or introduce new bugs. Each change should be hypothesis-driven: "I suspect X is causing the issue, so I will do Y to test/fix that."  
- **Repeated quick fixes without resolution:** If a particular issue keeps occurring (even after quick patches), don't just apply the same band-aid again. Investigate deeper. For instance, restarting a service to solve a memory leak is fine once, but doing it daily without fixing the leak is an antipattern. It creates complacency and technical debt.  
- **Silent failures:** Never catch an exception just to ignore it ("swallowing" exceptions) or return an error code without logging. This makes bugs hide. Instead, at least log the error with details. In production, silent failures can cascade into much larger issues. An example antipattern: a task fails internally but the system proceeds without output – now you have wrong data with no clue why. Always surface errors somewhere.  
- **Panic and blame:** In a crunch, pointing fingers or panicking wastes time. *"The Operating System is Fine."* – that is, assume external systems (OS, standard libraries) are probably correct. It's usually your code or misunderstanding. Don't default to "it must be a bug in the framework/OS" unless you have strong evidence. Even if it is, you need a workaround. And never attack colleagues; focus on solving the problem as a team.  
- **Debugging in production without safeguards:** Printing debug info or running a debugger on a production system can be risky. Avoid practices like enabling verbose debug logs in prod without bounds (filling up disk), or pausing a live process with a debugger (freezing service for users). If you must debug in prod, use tools designed for it (live tracing, or copy prod data to a staging environment to reproduce there).  
- **Not keeping context:** When debugging, avoid making changes while forgetting to restore context. Example: changing a config temporarily and not changing it back. Or testing a fix on your local machine with different environment variables and assuming it's fixed everywhere. Always track what context a test or fix was done under. Otherwise, you might think a bug is fixed when it was only hidden by a different environment.  
- **Ignoring small anomalies:** Small warnings or non-critical errors might be early signs of a bigger issue. Don't dismiss them. For instance, if logs show a non-fatal error happening frequently that "doesn't impact users (yet)", investigate it – it may uncover a logic error that could escalate.  
- **Poor logging practices:** Logging too little (e.g., just "Error occurred" without detail) or too much (dumping huge data indiscriminately) are both antipatterns. The former leaves you blind, the latter makes finding the useful info hard and can impact performance. Avoid gigantic single-line logs or logs without context. Log key events with relevant details (identifiers, state) at appropriate levels.  
- **No monitoring/alerting:** Not having monitors to catch anomalies (like error rate spikes, increased latency, high memory usage) is an antipattern in production environments. It leads to finding out problems only when users report them. Always have some form of automated eyes on your system, and avoid the pattern of relying purely on manual checks or luck to discover issues.

## Heuristics & Checklists

- **Log and trace first:** When an issue is reported, check logs and metrics at the time of the issue. 90% of the time, the logs will contain the clue (an exception stack trace, or a spike in a certain metric) that jumpstarts the investigation. Ensure you know where to find logs and that they have timestamps and severity levels.  
- **Recent changes:** Always ask "What changed recently?" A high percentage of bugs, especially production incidents, come from recent deployments or config changes. Use version control history, deployment logs, or a diff of config files to see what's new. If the bug wasn't there before a certain release, the diff of that release is prime suspect area.  
- **Binary search for failure cause:** If you have a multi-step process or a long function, use *binary chop* debugging. For instance, if data gets corrupted through a pipeline, check midway through: is it good or bad at that point? Keep halving the interval. Similarly, in source control, use `git bisect` to find which commit introduced the bug by testing halfway points. This methodical narrowing quickly pinpoints offending code.  
- **Minimal test case:** Try to extract a minimal test case that demonstrates the bug (particularly for algorithmic or data-related bugs). This might mean creating a small script or unit test that triggers the same failure. A minimal case removes noise and makes debugging focused. If it's not initially clear, iteratively simplify the input or state until you can't simplify further without the bug disappearing.  
- **Check invariants while debugging:** When stepping through code or analyzing logs, verify if the key invariants are holding at various points. If something that "should never happen" has happened by a certain line, you've discovered the where/when of the breach. Many debuggers allow adding watch expressions or conditional breakpoints – use them to pause when a variable goes out-of-range, etc.  
- **Dump state on error:** If an error is hard to reproduce locally, modify the code in production (or a staging environment) to dump extensive state when the error occurs. For example, on an unexpected exception, log the entire object or context. This one-time dump can give clues (like a configuration value or internal state) that weren't originally logged. Just be mindful of sensitive info – mask or avoid personal data in dumps.  
- **Don't forget environment factors:** Is the bug possibly caused by environment differences? (Different OS, Python version, 32-bit vs 64-bit, locale, timezone, etc.) Have a checklist of environment factors to compare between a working scenario and a failing scenario. Sometimes the "bug in code" is actually a difference in environment or configuration.  
- **Memory and resource leaks:** If the issue is performance degradation over time, consider leak possibilities. Use monitoring or profiling tools to see if memory, file handles, or other resources are growing without bound. If so, track allocations or use debugging tools (like Python's `tracemalloc` or C++ valgrind) to find where leaks occur.  
- **Ask for a second set of eyes:** If you're stuck, pair up with a colleague. Explain the problem and what you've found. Teaching it to someone often reveals something you overlooked. They might ask a question that leads to insight. Debugging by rubber-ducking (explaining step by step) or pair debugging can be much more effective than solo after a certain point.  
- **Restore and test after fix:** Once you think you have a fix, test it in an environment as close to prod as possible. If it's a data issue, use a copy of production data to verify the fix. Also, remove any extra logging or debug changes you added, and rollback any temporary measures (like manual hotfixes) to ensure the system is truly fixed and in a clean state.  
- **Update tests and monitors:** After resolving, add a regression test so that particular bug doesn't sneak back in. Also consider if you can add a monitor or alert for the future that would catch a similar issue sooner (e.g., if it was a silent data corruption, maybe add a periodic data consistency check).

## Key Quotes / Mnemonics

- "Don't Panic."  
- "Read the Damn Error Message."  
- "The Operating System is Fine."  
- "Reproduce It!"  
- "Fix the problem, not the blame."

## "When in doubt" Defaults

- **When in doubt, suspect your own code:** If you're unsure where a bug lies, default assumption should be "it's probably on our side." This will drive you to gather evidence and either confirm or eliminate your code as the cause. It prevents wild goose chases blaming externals that are usually stable.  
- **When in doubt, add logging:** If a part of the system is mysterious or non-transparent, add logging around it by default. Having more information is generally better than less when hunting a bug. You can always remove or reduce logs later.  
- **When in doubt, simplify the scenario:** Strip the problem down. Test under a simpler configuration, use a smaller dataset, or isolate the component. By removing variables, you reduce confusion. If the bug goes away when you simplify, add pieces back until it reappears – that last piece is the culprit.  
- **When in doubt, roll back or feature-flag off:** If a production issue is critical and you're not sure of the fix yet, the safest default action is to mitigate impact – e.g., roll back to a previous stable version, or turn off the feature via a flag. This buys time to properly diagnose without pressure. Always choose the known-good state over a risky quick change when users are on the line.  
- **When in doubt, take a break:** Debugging can be mentally exhausting. If you find yourself spinning in circles, a short break or a good night's sleep can reset your mind. Often, the solution becomes clear when you return with fresh eyes. As a default, if you've spent many hours with no progress, step away briefly – it often saves time in the long run.

## Production-Ready Context

- **Security Considerations:** When adding debug features, ensure they're properly secured. Debug endpoints should require authentication, verbose logging should not expose sensitive data.
- **Performance Impact:** Be mindful of the performance cost of instrumentation. Use sampling for high-frequency events, async logging for I/O-heavy operations.
- **Compliance:** In regulated environments, ensure debug logs comply with data retention and privacy policies. Consider what gets logged and for how long.

